#!/usr/bin/env bash
# Fetch file from a remote URL and verify its hash sum
# Usage:
# > fetch-verify FILENAME URL [sha1sum=SHA1SUM] [md5sum=MD5SUM]
# 
# SHA1SUM and MD5SUM may be URLs to a file that contains the checksum of the
# file to download and verify.
#
# Author: Jaeho Shin <netj@sparcs.org>
# Created: 2013-11-05
set -eu

[ $# -ge 2 ] || {
    sed -n '2,/^#$/ s/^# //p' <"$0"
    exit 2
}

File=$1; shift
URL=$1; shift
[ $# -eq 0 ] || declare "$@"
filename=$(basename "$File")

# special case URLs
case $URL in
    http://www.apache.org/dyn/closer.cgi*)
        case $File in *.html) ;; *)
            # The Apache dyn/closer.cgi gives an HTML page with a list of URLs to mirrors
            "$0" "$File".html "$URL"
            # We take the first URL and fetch-verify it instead
            URL=$(grep "http://.*/$File" "$File".html | head -1 | sed -e 's/.*href="//; s/".*//')
            rm -f "$File".html
            exec "$0" "$File" "$URL" "$@"
        esac
esac

# expand any URL for check sums
for sum in sha1sum md5sum; do
    sumUrl=${!sum:-}
    case $sumUrl in
        http://*|https://*)
            case $sum in
                sha1sum) len=40 ;;
                 md5sum) len=32 ;;
            esac
            # find checksum lines from the contents of the URL
            value=$({ curl -sS "$sumUrl" || wget -qO- "$sumUrl"
                    } | grep '[0-9a-fA-F]\{'$len'\}')
            if ! [[ ${#value} -eq $len ]]; then
                # unless the contents is exactly the length of the checksum,
                # find the lines that mention the file name
                value=$(
                    grep -F "$filename" <<<"$value" |
                    sed 's/.*\([0-9a-fA-F]\{'$len'\}\).*/\1/'
                )
            fi
            declare "$sum=$value"
    esac
done

tag=${sha1sum:-${md5sum:-}}
cached="${BUILDKIT_CACHE_DIR:=${TMPDIR:-/tmp}/buildkit-$USER/cache}/${tag:+${tag}.}$filename"
cachedir=$(dirname "$cached")

# aliases for checking hash sums
shasum=${sha1sum:-}
md5=${md5sum:-}

fileOK() {
    local fileOK=false
    if [ -s "$File" ]; then
        fileOK=true # assume file is okay unless it's known to be corrupt
        for hashType in sha256sum sha1sum shasum md5sum md5; do
            for hash in ${!hashType:-}; do
                [[ -n "$hash" ]] && type "$hashType" &>/dev/null || continue
                fileOK=false # assume corrupted when we got a hash sum that can be checked
                if "$hashType" <"$File" | grep -qF "$hash"; then
                    fileOK=true # and until we found a correct hash sum
                    break
                fi
                ! $fileOK || break
            done
        done
    fi
    $fileOK
}
fileOK ||
    { [[ -e "$cached" ]] && cp -f "$cached" "$File"                  && fileOK; } ||
    { curl --continue-at - --location --fail --output "$File" "$URL" && fileOK; } ||
    { curl --location --fail --output "$File" "$URL"                 && fileOK; } ||
    { wget --continue --output-document="$File" "$URL"        && fileOK; } ||
    { wget --output-document="$File" "$URL"                   && fileOK; } ||
    { echo >&2 "Cannot fetch $URL as $File"                   && fileOK; }


# cache for later (not critical)
set +e
mkdir -p "$cachedir"
chmod go= "$cachedir"
cp -f "$File" "$cached"
